# -*- coding: utf-8 -*-
"""MOVIE GENRE CLASSIFICATION.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1P4UZDBhDBomeT2_W1fPVH22k8S0AeRas
"""

import os
import sys
from tempfile import NamedTemporaryFile
from urllib.request import urlopen
from urllib.parse import unquote, urlparse
from urllib.error import HTTPError
from zipfile import ZipFile
import tarfile
import shutil

CHUNK_SIZE = 40960
DATA_SOURCE_MAPPING = 'genre-classification-dataset-imdb:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F1417162%2F2347441%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240212%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240212T102744Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D53335b037651c658013a6e1f37398e672b9a752f9f2a8e50bf6aed031c656953bc9697da9ab0bbf5fc1e8810752281cef6af16b299440bba79aef214f64542f346395e381760b2803e8213d65057cafe5850238d2045e50ac8c61668e058ef1a28e955c08f190dcbf06859f80255cdb84818063be20cae1f752f4b0bf704e0a533ccea58399d98a314bf2f5181b5c0d36a8536639fffad0a05045125a69f19d347905cbaecaececb483dc83a4bb7695d7daf8d5466fa2784196c31148b4f58b56f7d6429c9610f07d3bc13654809b189a1f1d2f0af0a7c25a223d983fc57277656ce04ccdfc8912d9324568972f576cd9e6cd1aa45233af8c3a5f2c290fd6cbc'

KAGGLE_INPUT_PATH='/kaggle/input'
KAGGLE_WORKING_PATH='/kaggle/working'
KAGGLE_SYMLINK='kaggle'

!umount /kaggle/input/ 2> /dev/null
shutil.rmtree('/kaggle/input', ignore_errors=True)
os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)
os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)

try:
  os.symlink(KAGGLE_INPUT_PATH, os.path.join("..", 'input'), target_is_directory=True)
except FileExistsError:
  pass
try:
  os.symlink(KAGGLE_WORKING_PATH, os.path.join("..", 'working'), target_is_directory=True)
except FileExistsError:
  pass

for data_source_mapping in DATA_SOURCE_MAPPING.split(','):
    directory, download_url_encoded = data_source_mapping.split(':')
    download_url = unquote(download_url_encoded)
    filename = urlparse(download_url).path
    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)
    try:
        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:
            total_length = fileres.headers['content-length']
            print(f'Downloading {directory}, {total_length} bytes compressed')
            dl = 0
            data = fileres.read(CHUNK_SIZE)
            while len(data) > 0:
                dl += len(data)
                tfile.write(data)
                done = int(50 * dl / int(total_length))
                sys.stdout.write(f"\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded")
                sys.stdout.flush()
                data = fileres.read(CHUNK_SIZE)
            if filename.endswith('.zip'):
              with ZipFile(tfile) as zfile:
                zfile.extractall(destination_path)
            else:
              with tarfile.open(tfile.name) as tarfile:
                tarfile.extractall(destination_path)
            print(f'\nDownloaded and uncompressed: {directory}')
    except HTTPError as e:
        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')
        continue
    except OSError as e:
        print(f'Failed to load {download_url} to path {destination_path}')
        continue

print('Data source import complete.')

import pandas as pd
from transformers import BertModel, BertTokenizerFast, TFBertModel
import torch
import tensorflow as tf

device = "cuda:0" if torch.cuda.is_available() else "cpu"
device

# train_data = pd.read_csv('archive/Genre Classification Dataset/train_data.txt', sep=':::', header=None, index_col=0)
train_data = pd.read_csv("/kaggle/input/genre-classification-dataset-imdb/Genre Classification Dataset/train_data.txt", delimiter=':::', header=None, engine='python')
test_data = pd.read_csv("/kaggle/input/genre-classification-dataset-imdb/Genre Classification Dataset/test_data.txt", delimiter=':::', header=None, engine='python')
test_data_solution = pd.read_csv("/kaggle/input/genre-classification-dataset-imdb/Genre Classification Dataset/test_data_solution.txt", delimiter=':::', header=None, engine='python')

train_data.head()

# counting genres
genres = train_data[2].unique().tolist()
len(genres)

# visualize label counts
train_data[2].value_counts().plot(kind='pie', figsize=(10,10));

x_train, y_train = train_data.drop(2, axis=1), train_data.iloc[:, 2]

x_train.head()

"""# Tokenization"""

tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')
# tokenizer = tokenizer.to(device)

descriptions = list(x_train[3])
descriptions[0]

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# encoded_dicts = []
# 
# for description in descriptions[:10]:
#     encoded_dicts.append(tokenizer.encode_plus(
#                         description,                    # Sentence to split into tokens
#                         truncation=True,                # cut longer strings
#                         add_special_tokens = True,      # Add special token '[CLS]' and '[SEP]'
#                         max_length = 256,               # Pad & truncate all sentences.
#                         pad_to_max_length = True,       # fill shorter strings
#                         return_attention_mask = True,   # Construct attention masks.
# #                         return_tensors = 'pt',          # Return pytorch tensors.
#                    ))
# 
# encoded_dicts[0]

# Commented out IPython magic to ensure Python compatibility.
# %%time
# model = BertModel.from_pretrained("bert-base-uncased",).to(device)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# data_loader = torch.utils.data.DataLoader(encoded_items, batch_size=16, shuffle=False,)

# %%time
for batch in data_loader:
    input_ids = batch['input_ids'].to(device)
    attention_mask = batch['attention_mask'].to(device)
    outputs = model(input_ids=input_ids, attention_mask=attention_mask)
    print(outputs)
    break

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# outputs = []
# 
# for dic in encoded_dicts:
#     outputs.append(model(**dic))

df.DataFrame(outputs).to_csv('/kaggle/working/output.csv')

"""# Embedding"""

# !pip install simpletransformers
from simpletransformers.language_representation import RepresentationModel
model = RepresentationModel(
        model_type="bert",
        model_name="bert-base-uncased",
        use_cuda=True
    )

# Commented out IPython magic to ensure Python compatibility.
# %%time
# sentence_vectors = model.encode_sentences(descriptions, combine_strategy="mean")

sentence_vectors.shape

"""# Training NN"""

import pandas as pd
from matplotlib import pyplot as plt
import numpy as np

import torch
import torch.nn as nn
import torch.nn.functional as fun
from torchvision import datasets, transforms, models
from torch.utils.data import DataLoader

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

class NN(nn.Module):
    def __init__(self, input_shape, output_shape):
        super(NN, self).__init__()

        hidden1 = 4096
        hidden2 = 2048
        hidden3 = 512

        self.fc1 = nn.Linear(input_shape, hidden1)
        self.fc2 = nn.Linear(hidden1, hidden2)
        self.fc3 = nn.Linear(hidden2, hidden3)
        self.fc4 = nn.Linear(hidden3, output_shape)

    def forward(self, x):
        x = fun.relu(self.fc1(x))
        x = fun.relu(self.fc2(x))
        x = fun.relu(self.fc3(x))
        x = self.fc4(x)
        return x

network = NN(input_shape=sentence_vectors.shape[1],output_shape=len(genres))

def train(model, epochs, optimizer, loss_fun, loss_history, accuracy_history, vector = True):
  model.train()
  model.to(device)

  for epoch in range(epochs):
    print("epoch: ", epoch+1)

    for ind, (data, targets) in enumerate(train_loader):

      data = data.to(device)
      targets = targets.to(device)

      if vector:
        data = vectorize(data)

      pred = model(data)
      loss = loss_fun(pred, targets)
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()

      if ind % 200 == 0:
        acc = get_accuracy(model, test_loader, vector)
        print("batch: ", ind)
        print("train loss: ", loss.item())
        print("test accuracy", acc)
        accuracy_history.append(acc)
        loss_history.append(loss.item())

def get_accuracy(model, loader, vector=True):
    model.eval()
    model.to(device)

    total_samples = 0
    total_hits = 0

    for x, y in loader:

        # stack the RGB values together
        y = y.to(device)
        x = x.to(device)

        if vector:
          x = vectorize(x)


        with torch.no_grad():
            pred = model(x)
            pred = torch.argmax(pred, dim=1)
            total_hits += torch.sum(pred == y).item()
            total_samples += y.shape[0]

    return total_hits/total_samples

def get_prediction(model, loader, vector=True):
    model.eval()
    model.to(device)

    full_pred = torch.tensor([], device=device)
    full_targets = torch.tensor([], device=device)



    for x, y in loader:
        # print(x.shape)
        if vector:
          x = vectorize(x)

        x = x.to(device)
        y = y.to(device)

        with torch.no_grad():
            y_pred = model(x)
            y_pred = torch.argmax(y_pred, dim=1)

            full_pred = torch.cat((full_pred, y_pred), 0)
            full_targets = torch.cat((full_targets, y), 0)

    return full_pred.to("cpu"), full_targets.to("cpu")

def print_pred(model, samples, loader, vector=True):

    rows, cols = 1, samples
    fig, ax = plt.subplots(rows, cols, figsize=(15,5))

    for _ in range(samples):

        batch = next(iter(loader))
        ind = torch.randint(1, batch.size[0])
        img, label = batch[ind]

        x = img
        if vector:
            x = vectorize(img)

        with torch.no_grad():
            pred = model(x)
            pred = torch.argmax(pred, dim=1)


        for i in range(cols):
            ind = torch.randint(low=0, high=50000, size=(1,))
            img, label = train_data[ind]
            img = torch.stack((img[0], img[1], img[2]), dim=-1)

            ax[i].imshow(img)
            ax[i].axis("off")
            ax[i].set_title(class_map[pred])

train_data = np.hstack((sentence_vectors, np.array(y_train).reshape(-1,1)))

train_data.shape

train_loader =